<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="description" content="">
	<meta name="author" content="Thushan Fernando">

	<title>DeepSeek DeepInsights</title>

	<link rel="stylesheet" href="engine/reset.css">
	<link rel="stylesheet" href="engine/reveal.css">
	<link rel="stylesheet" href="engine/thushan.css">
	<link rel="stylesheet" href="engine/theme/black.css" id="theme">
	<link rel="stylesheet" href="engine/plugin/highlight/monokai.css" id="highlight-theme">
</head>
<body>
	<div class="reveal">
		<div class="slides">
			<section data-markdown>
				## DeepSeek
				## What's all the fuss about?
				Express Tour

				![Splash Image](assets/img/deepseek-techbro.gif)

				[github.com/thushan](https://github.com/thushan)
			</section>
			<section>
				<section class="img_container">
					<div class="tweet" data-src="https://twitter.com/deepseek_ai/status/1881318130334814301"></div>
				</section>
				<section class="img_container">
					<img src="assets/img/deepseek-bench.png" class="fade-in fade-out">
				</section>
				<section>
					<img src="assets/img/few-moments-later.gif" class="fade-in fade-out">
				</section>  
				<section>
					<img src="assets/img/homer-stock-drop.gif" class="fade-in fade-out">
				</section>  
				<section class="img_container">
					<img src="assets/img/nvidia-stock-drop.jpg" alt="Nvidia Stock Drop" class="fade-in fade-out" width="70%">					
				</section>
				<section>
					<img src="assets/img/monty-money-gone.gif" class="fade-in">
				</section>
				<section>
					<h2>There's more to the story</h2>
					<img src="assets/img/nvidia-shareprice.png" class="fade-in">
				</section>
				<section>
					<h2>Couple of months earlier...</h2>
					<img src="assets/img/deepseek-panic.jpg" class="fade-in">
				</section>
			</section>
			<section>
				<section>
					<h2>Who are DeepSeek?</h2>
					<ul>
						<li class="fragment">Founded 2016 by Liang Wengfeng (1985)</li>
						<li class="fragment">Privately owned & funded by High-Flyer ($8B HF)</li>
						<li class="fragment">~160-180 Employees (as of early-2025)</li>
						<li class="fragment">Hires for ability over experience - graduates, PhDs</li>
						<li class="fragment">Meritocratic culture - creativity, passion & collabs</li>
						<li class="fragment"><strong>LLMs:</strong> DeepSeek Coder (2023), DeepSeek v2 (2024)</li>
						<li class="fragment">2019 established AI Trading algorithms (Fire-Flyer)</li>
						<li class="fragment">2022 Fire-Flyer 2 (5000 A100 GPUs)</li>
						<li class="fragment">Focused on R&D into LLMs, no commercialisation</li>
						<li class="fragment">That avoids China's Gov AI Controls & Regulations</li>						
					</ul>
				</section>
				<section>
					<h2>What makes them special?</h2>
					<ol>
						<li class="fragment">Priortise <strong>efficiency & cost-effectiveness</strong></li>
						<li class="fragment">Innovations in architecture & approach</li>
						<li class="fragment">MIT License - the most open & permissive</li>
						<li class="fragment">Extensive Papers with each release:
							<small>
								<ol class="fragment fade-in">
									<li>DeepSeek Coder (November 2, 2023)</li>
									<li>DeepSeek-LLM series (November 29, 2023)
									<li>DeepSeek-MoE models (January 9, 2024)</li>
									<li>DeepSeek-Math models (April 2024)</li>
									<li>DeepSeek-V2 (May 2024)</li>
									<li>DeepSeek-Coder V2 series (June 2024)</li>
									<li>DeepSeek V2.5 (September 2024, updated in December)</li>
									<li>DeepSeek-R1-Lite-Preview (November 20, 2024)</li>
									<li>DeepSeek-V3-Base and DeepSeek-V3 (December 2024) **</li>
									<li>DeepSeek-R1 (January 20, 2025) *</li>
								</ol>
							</small>
						</li>
					</ul>
				</section>
				<section>
					<h2>How do they compare?</h2>
					<small>
					<table>
						<thead>
						   <tr>
							  <th>Feature</th>
							  <th>DeepSeek R1</th>
							  <th>OpenAI o1</th>
							  <th>Claude 3.7 Son</th>
							  <th>Llama 3.3</th>
						   </tr>
						</thead>
						<tbody>
						   <tr>
							  <td>License</td>
							  <td class="glowing-text-blue">MIT OSS</td>
							  <td>Proprietary</td>
							  <td>Proprietary</td>
							  <td>OSS <small>(Non-Commercial)</small></td>
						   </tr>
						   <tr>
							  <td>Architecture</td>
							  <td class="glowing-text-neon">Mixture of Experts (MoE)</td>
							  <td>Transformer-based</td>
							  <td>Transformer-based</td>
							  <td>Transformer-based</td>
						   </tr>
						   <tr>
							  <td>Parameters</td>
							  <td class="glowing-text-yellow">671B (37B act)</td>
							  <td>Not disclosed</td>
							  <td>Not disclosed</td>
							  <td>405B</td>
						   </tr>
						   <tr>
							  <td>Cost</td>
							  <td class="glowing-text-red">~$5.6M</td>
							  <td>$100M+ (est)</td>
							  <td>Not disclosed</td>
							  <td>Not disclosed</td>
						   </tr>
						   <td>Context</td>
						   <td>128K tokens</td>
						   <td>100K tokens</td>
						   <td>200K tokens</td>
						   <td>Not disclosed</td>
						   </tr> 
						   <tr>
							  <td>Strengths</td>
							  <td>
								<ul>
									<li>Cost-efficiency</li>
									<li>Reasoning</li>
								</ul>
							</td>
							  <td>
								<ul>
									<li>Performance</li>
									<li>Scalability</li>
								</ul>
							</td>
							  <td>
								<ul>
									<li>Safety</li>
									<li>Extended Thinking</li>
								</ul></td>
							  <td>
								<ul>
									<li>Multilingual (8)</li>
									<li>Zero-shot Tuned</li>
								</ul>
							</td>
						   </tr>
						</tbody>
					 </table>
					</small>
				</section>
				<!-- remove background info -->				
			</section>
			<section>
				<section>
					<h2>DeepSeek V3</h2>
					<img src="assets/img/deepseek_v3_example_en.gif" class="fade-in">
				</section>
				<section>
					<h2>DeepSeek v3</h2>
					<ul>
						<li class="fragment">Released December 26, 2024</li>
						<li class="fragment">617B MoE Architecture <span class="fragment"> - uses 37B per query</span></li>
						<li class="fragment">Heavily optimised for Code</li>
						<li class="fragment">Foundations built for Reasoning</li>
						<li class="fragment">chat.deepseek.com - no limits, subscriptions</li>
						<li class="fragment">Very competitive API rates:
							<small>
								<ul>
									<li>$0.27/MT (cache miss)</li>
									<li>$0.07/MT (cache hit)</li>
									<li>$1.10/MT Output</li>
								</ul>
							</small>
						</li>
					</ul>
				</section>
				<section>
					<h2>DeepSeek R1</h2>
					<img src="assets/img/deepseek_r1_example_en.gif" class="fade-in">
				</section>
				<section>
					<h2>DeepSeek R1</h2>
					<ul>						
						<li class="fragment">Released January 20, 2025</li>
						<li class="fragment">First MIT/OSS Reasoning Model</li>
						<li class="fragment">32B, 70B models on par with OpenAI o1-mini</li>
						<li class="fragment">Heavily optimised for Code & Math</li>
						<li class="fragment">Reinforcement Learning Approach</li>
					</ul>
				</section>
			</section>
			<section>
				<h2>DeepSeek Requirements</h2>
				<ul>
					<li class="fragment">671B can run on 1.5TB of RAM</li>
					<li class="fragment">vs LLama 70B needs 40-150GB of VRAM</li>
					<li class="fragment">Demonstrated LLMs on commodity hardware</li>
					<li class="fragment">You don't even need GPU + lots of VRAM</li>
					<li class="fragment">Performs slower, but with <code>exa</code> + GPUs flies</li>
				</ul>
			</section>
			<section>
				<h2>DeepSeek Technologies</h2>
				<ul>
					<li class="fragment">MoE: Mixture of Experts</li>
					<li class="fragment">MLA: Multi-Head Latent Attention</li>
					<li class="fragment">Reinforcement Learning</li>
				</ul>
				<p class="fragment">All backed with papers from R&D (arXiv)</p>
				<p class="fragment">Engineered to run on non-GPU hardware</p>
				<p class="fragment">Custom Silicon for MoE Computation</p>
			</section>
			<section>
				<section>
					<h2>MoE? Moe Szyslak?</h2>
					<img src="assets/img/moes-tavern.gif" class="fade-in">
					<p class="fragment">Not quite, but it's a bar for experts</p>
				</section>
				<section>
					<h2>MoE: Mixture of Experts</h2>
					<ul>
						<li class="fragment">Activates '37B expert' neurons <small>(of 671B)</small>per query</li>
						<li class="fragment">Imagine a library with 671B books (full dense model)</li>
						<li class="fragment">If you ask a question, instead of looking through 607B books...</li>
						<li class="fragment">...only the most relevant 37B books to answer are activated</li>
					</ul>
				</section>
				<section>
					<h2>MoE: Mixture of Experts</h2>
					<ul>
						<li class="fragment">DeepSeek solved the 'expert collapse' problem where models tend to overuse certain experts while ignoring others</li>
						<li class="fragment">Introduced dynamic bias terms for expert routing and separating experts into shared and routed categories</li>
						<li class="fragment">3x faster than GPT-4 at ~1/20th the training cost ($6M vs $100M)</li>
					</ul>
				</section>
				<section>
					<h3>They've done studies you know...</h3>
					<img src="assets/img/deep-seek-anchorman.jpg" class="fade-in">
					<p class="fragment">It's also illegal in 9 countries... maybe.</p>
				</section>
				<section>
					<h2>MoE: Examples</h2>
					<ul>
						<li class="fragment">Google Gemni 1.5</li>
						<li class="fragment">Mistral Mixtral 8x7B</li>
						<li class="fragment">DeepSeek v3 (671B Model, 37B activations)</li>
						<li class="fragment">Google GLaM (1.2T Model, 97B activations)</li>
						<li class="fragment">Google T5 MoE (1.2T Model, Adaptive expert activations)</li>
					</ul>
				</section>
				<section>
					<h2>Dense Models: Examples</h2>
					<ul>
						<li class="fragment">OpenAI GPT-4, GPT-3 (175B)</li>
						<li class="fragment">Anthropic Claude 3 Opus</li>
						<li class="fragment">Meta Llama 2, Llama 3</li>
						<li class="fragment">Mistral 7B</li>
					</ul>
					<p class="fragment">Dense models provide stable quality</p>
					<p class="fragment">...but costly in compute, uses all params.</p>
				</section>
				<section>
					<h2>Optimised Activations</h2>					
					<ul>
						<li class="fragment">What if my query only needs 10B Params?</li>
						<li class="fragment">MoE (DeepSeek v3): Still activates 37B irrespective</li>
						<li class="fragment">Dense (GPT-3): Still activates 175B regardless</li>
						<li class="fragment"><strong>Dynamic Routing:</strong> a more efficient MoE, dynamically selects activations based on query</li>
						<li class="fragment">Super complex to implement & get right, often only for T's of params</li>
						<li class="fragment">Like Google - GShard (2020), GLaM (2021), T5 Moe (2022) or OpenMoe (2023)</li>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h2>Multi-Head Latent Attention (MLA)</h2>
					<ul>
						<li class="fragment">Introduced in DeepSeek v2</li>
						<li class="fragment">Compresses KV Matrices into latent vectors</li>
						<li class="fragment">Can reduce ~92-95% in memory usage bottlenecks</li>
						<li class="fragment">Allows longer context-windows (128K for instance)</li>
						<li class="fragment">KV Cache is usually a rolling buffer, inference side</li>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h2>Reinforcement Learning</h2>
					<ul>
						<li class="fragment">Trains models through trial-and-error, rewarding correct reasoning steps and answers</li>
						<li class="fragment">Reduces reliance on large labeled datasets, addressing data privacy and bias concerns</li>
						<li class="fragment">Enables autonomous development of chain-of-thought reasoning and self-verification</li>
						<li class="fragment">Uses Group Relative Policy Optimisation for efficient training without a separate critic model</li>
						<li class="fragment">Allows models to adapt and refine strategies, leading to improved problem-solving abilities</li>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h2>Some Lols & OMG were had</h2>
					<ul>
						<li class="fragment">DeepSeek sometimes said it was GPT4, sometimes LLama 3 - Identity Confusion</li>
						<li class="fragment">DeepSeek R1 when thinking about inappropriate language/words would explore similar tone before refusing to answer.</li>
						<li class="fragment">DeepSeek was touchy on topics (Tiananmen Square/Taiwan) & realtime censoring in R1</li>
					</ul>
				</section>
				<section>
					<h2>Some Security Concerns</h2>
					<ul>
						<li class="fragment">100% attack success rate in security tests</li>
						<li class="fragment">Susceptible to <a href="https://www.kelacyber.com/blog/deepseek-r1-security-flaws/">'Evil Jailbreak'</a> method</li>
						<li class="fragment">Capable of generating harmful content and malicious code</li>
						<li class="fragment">Australia Banned the use of DeepSeek (hosted)</li>
						<li class="fragment">UK Gov advised against it for <a href="https://www.theguardian.com/technology/2025/jan/28/experts-urge-caution-over-use-of-chinese-ai-deepseek">spreading misinformation</a></li>
						<li class="fragment">Hosted DeepSeek kept user behaviour/data in an open ClickHouse Instance</li>
						<li class="fragment">Hosted DeepSeek may divulge data to Chinese agencies required by law</li>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h2>What's the future</h2>
					<img src="assets/img/byd-deepseek.png" class="fragment fade-in" />
					<p class="fragment">BYD + others & DeepSeek for autonomous driving</p>
				</section>
				<section>
					<h2>What's the future</h2>
					<ul>
						<li class="fragment">Cloud vendors adopt V3 and R1 - Azure / AWS</li>
						<li class="fragment">Perplexity switched to R1 & saved ~96% costs</li>
						<li class="fragment">Microsoft ported (distilled) <a href="https://www.pcworld.com/article/2593957/microsoft-ports-deepseeks-ai-to-copilot-pcs-and-their-npus.html">Models to CoPilot+ PCs</a></li>
						<li class="fragment">Both AMD & NVIDIA are optimising hardware to <a href="https://www.scmp.com/tech/big-tech/article/3302895/amd-ceo-lisa-su-visits-china-touting-ai-chip-compatibility-deepseek-alibaba-models">run DeepSeek models</a></li>
						<li class="fragment">DeepSeek v3.5 (2026) & DeepSeek R2 (2027) in the works</li>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h2>How to play with DeepSeek?</h2>
					<ul>
						<li class="fragment">Install <a href="https://ollama.com/">Ollama</a> + <a href="https://github.com/open-webui/open-webui">OpenWebUI</a> or Grab <a href="https://docs.openwebui.com/getting-started/quick-start">Docker Image</a></li>
						<li class="fragment">Find <a href="https://ollama.com/library/deepseek-r1">DeepSeek R1</a> in Library</li>
						<li class="fragment">Pick a distilled model - Qwen-14B:<br/><code>ollama run deepseek-r1:14b</code></li>
						<li class="fragment">Or use <a href="https://openrouter.ai/deepseek/deepseek-r1:free">OpenRouter DeepSeek:R1</a> (free)</li>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h2>Just one more thing...</h2>
					<ul>
						<li class="fragment">Take a look at <a href="https://huggingface.co/Qwen">Qwen on HF</a></li>
						<li class="fragment">It's the model family built by AliCloud</li>
						<li class="fragment">ERNIE 4.5 / X1 from Baidu <a href="https://www.prnewswire.com/news-releases/baidu-unveils-ernie-4-5-and-reasoning-model-ernie-x1--makes-ernie-bot-free-ahead-of-schedule-302402490.html">was released yesterday</a>!</li>
					</ul>
				</section>
				<section>
					<h2>References</h2>
					<ul>
						<li><a href="https://www.deepseek.com/papers">DeepSeek Papers</a></li>
						<li><a href="https://www.deepseek.com/blog">DeepSeek Blog</a></li>
						<li><a href="https://api-docs.deepseek.com/news/news1226">DeepSeek v3</a></li>
						<li><a href="https://api-docs.deepseek.com/news/news250120">DeepSeek R1</a></li>
						<li><a href="https://towardsai.net/p/artificial-intelligence/a-visual-walkthrough-of-deepseeks-multi-head-latent-attention-mla-%EF%B8%8F">Visual Walkthrough of DeepSeeks MLA</a></li>
						<li><a href="https://medium.com/data-science/deepseek-v3-explained-1-multi-head-latent-attention-ed6bee2a67c4">DeepSeek v3 MLA Explained</a></li>
						<li><a href="https://www.youtube.com/watch?v=0VLAoVGf_74">YT: How DeepSeek rewrote The Transformer</a></li>
					</ul>
				</section>
			</section>
		</div>
	</div>

	<script src="engine/reveal.js"></script>
	<script src="engine/plugin/chart/chart.min.js"></script>
	<script src="engine/plugin/chart/plugin.js"></script>
	<script src="engine/plugin/chalkboard/plugin.js"></script>
	<script src="engine/plugin/notes/notes.js"></script>
	<script src="engine/plugin/markdown/markdown.js"></script>
	<script src="engine/plugin/highlight/highlight.js"></script>
	<script src="engine/plugin/mermaid/mermaid.js"></script>
	<script src="engine/plugin/ember/plugin.js"></script>
	<script>
		Reveal.initialize({
			hash: true,
			chart: {
				defaults: {
					global: {
						title: { fontColor: "#FFF" },
						legend: {
							labels: { fontColor: "#FFF" },
						},
					},
					scale: {
						scaleLabel: { fontColor: "#FFF" },
						gridLines: { color: "#FFF", zeroLineColor: "#FFF" },
						ticks: { fontColor: "#FFF" },
					}
				},
				line: { borderColor: ["rgba(20,220,220,.8)", "rgba(220,120,120,.8)", "rgba(20,120,220,.8)"], "borderDash": [[5, 10], [0, 0]] },
				bar: { backgroundColor: ["rgba(20,220,220,.8)", "rgba(220,120,120,.8)", "rgba(20,120,220,.8)"] },
				pie: { backgroundColor: [["rgba(0,0,0,.8)", "rgba(220,20,20,.8)", "rgba(20,220,20,.8)", "rgba(220,220,20,.8)", "rgba(20,20,220,.8)"]] },
				radar: { borderColor: ["rgba(20,220,220,.8)", "rgba(220,120,120,.8)", "rgba(20,120,220,.8)"] },
			},
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealChart, RevealChalkboard, RevealMermaid, RevealEmbedTweet]
		});
	</script>
</body>

</html>
